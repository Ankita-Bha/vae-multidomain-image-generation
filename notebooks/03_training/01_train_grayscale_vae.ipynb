{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf0da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current = Path().resolve()\n",
    "while not (current / \"src\").exists():\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.append(str(current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8355cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from src.models.vae import ConvVAE\n",
    "from src.training.losses import vae_loss\n",
    "from src.training.scheduler import BetaScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13893197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on MNIST\n",
      "mnist | Epoch 1 | Loss 3844250\n",
      "mnist | Epoch 2 | Loss 2315004\n",
      "mnist | Epoch 3 | Loss 2784911\n",
      "mnist | Epoch 4 | Loss 3186971\n",
      "mnist | Epoch 5 | Loss 3550053\n",
      "\n",
      "Training on FASHION\n",
      "fashion | Epoch 1 | Loss 3730620\n",
      "fashion | Epoch 2 | Loss 2940892\n",
      "fashion | Epoch 3 | Loss 3212252\n",
      "fashion | Epoch 4 | Loss 3457891\n",
      "fashion | Epoch 5 | Loss 3674759\n",
      "\n",
      "Training on EMNIST\n",
      "emnist | Epoch 1 | Loss 6481904\n",
      "emnist | Epoch 2 | Loss 6142954\n",
      "emnist | Epoch 3 | Loss 7715624\n",
      "emnist | Epoch 4 | Loss 8091581\n",
      "emnist | Epoch 5 | Loss 7957298\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.grayscale_datasets import get_grayscale_loader\n",
    "\n",
    "datasets_to_train = [\"mnist\", \"fashion\", \"emnist\"]\n",
    "\n",
    "for ds in datasets_to_train:\n",
    "    print(f\"\\nTraining on {ds.upper()}\")\n",
    "\n",
    "    loader = get_grayscale_loader(\n",
    "        dataset_name=ds,\n",
    "        root=current / \"data\" / \"raw\",\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "    model = ConvVAE(latent_dim=32).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    beta_scheduler = BetaScheduler(0.0, 1.0, 5000)\n",
    "\n",
    "    epochs = 5\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x)\n",
    "            beta = beta_scheduler.step()\n",
    "            loss, _, _ = vae_loss(recon, x, mu, logvar, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"{ds} | Epoch {epoch+1} | Loss {total_loss:.0f}\")\n",
    "\n",
    "    # save model\n",
    "    ckpt_dir = current / \"checkpoints\" / \"grayscale\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), ckpt_dir / f\"vae_{ds}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5ca7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SHARP VAE on FASHIONMNIST\n",
      "Using device: cpu\n",
      "Epoch [1/15] | Loss: 3420405 | Recon: 3175921 | KL: 4889688 | Beta: 0.05\n",
      "Epoch [2/15] | Loss: 1978993 | Recon: 1728571 | KL: 5008446 | Beta: 0.05\n",
      "Epoch [3/15] | Loss: 1807790 | Recon: 1557618 | KL: 5003438 | Beta: 0.05\n",
      "Epoch [4/15] | Loss: 1723000 | Recon: 1473196 | KL: 4996096 | Beta: 0.05\n",
      "Epoch [5/15] | Loss: 1667348 | Recon: 1418137 | KL: 4984232 | Beta: 0.05\n",
      "Epoch [6/15] | Loss: 1629582 | Recon: 1381422 | KL: 4963193 | Beta: 0.05\n",
      "Epoch [7/15] | Loss: 1599350 | Recon: 1351673 | KL: 4953536 | Beta: 0.05\n",
      "Epoch [8/15] | Loss: 1575049 | Recon: 1328196 | KL: 4937065 | Beta: 0.05\n",
      "Epoch [9/15] | Loss: 1556721 | Recon: 1310554 | KL: 4923341 | Beta: 0.05\n",
      "Epoch [10/15] | Loss: 1539980 | Recon: 1294222 | KL: 4915160 | Beta: 0.05\n",
      "Epoch [11/15] | Loss: 1526421 | Recon: 1281038 | KL: 4907648 | Beta: 0.05\n",
      "Epoch [12/15] | Loss: 1514304 | Recon: 1269125 | KL: 4903598 | Beta: 0.05\n",
      "Epoch [13/15] | Loss: 1502201 | Recon: 1257314 | KL: 4897739 | Beta: 0.05\n",
      "Epoch [14/15] | Loss: 1491726 | Recon: 1247136 | KL: 4891783 | Beta: 0.05\n",
      "Epoch [15/15] | Loss: 1484077 | Recon: 1239767 | KL: 4886191 | Beta: 0.05\n",
      "Saved: checkpoints/grayscale/vae_fashion_sharp.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FashionMNIST â€” Sharper VAE (Demo-biased training)\n",
    "# Purpose:\n",
    "# - Improve visual quality for demo\n",
    "# - Keep same latent space + traversal\n",
    "# - Used for Streamlit comparison\n",
    "# ============================================================\n",
    "\n",
    "# ---- FIX: make sure `src/` is importable in Jupyter ----\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current = Path().resolve()\n",
    "while not (current / \"src\").exists():\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from src.datasets.grayscale_datasets import get_grayscale_loader\n",
    "from src.models.vae import ConvVAE\n",
    "from src.training.losses import vae_loss\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "\n",
    "print(\"\\nTraining SHARP VAE on FASHIONMNIST\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load ONLY FashionMNIST\n",
    "loader = get_grayscale_loader(\n",
    "    dataset_name=\"fashion\",\n",
    "    root=current / \"data\" / \"raw\",\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Model + optimizer\n",
    "model = ConvVAE(latent_dim=32).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Fixed low beta for sharper images\n",
    "beta = 0.05   # <-- key change\n",
    "\n",
    "epochs = 15   # reasonable time, visible improvement\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kl = 0.0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = model(x)\n",
    "        loss, recon_loss, kl = vae_loss(\n",
    "            recon, x, mu, logvar, beta\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kl += kl.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "        f\"Loss: {total_loss:.0f} | \"\n",
    "        f\"Recon: {total_recon:.0f} | \"\n",
    "        f\"KL: {total_kl:.0f} | \"\n",
    "        f\"Beta: {beta}\"\n",
    "    )\n",
    "\n",
    "# Save as NEW checkpoint (do not overwrite vanilla VAE)\n",
    "ckpt_dir = current / \"checkpoints\" / \"grayscale\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    ckpt_dir / \"vae_fashion_sharp.pt\"\n",
    ")\n",
    "\n",
    "print(\"Saved: checkpoints/grayscale/vae_fashion_sharp.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
