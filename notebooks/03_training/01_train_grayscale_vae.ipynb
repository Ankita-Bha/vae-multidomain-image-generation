{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80693d67",
   "metadata": {},
   "source": [
    "# VAE Training\n",
    "\n",
    "Purpose: Train the VAE on grayscale datasets.\n",
    "\n",
    "Includes:\n",
    "- Reconstruction + KL loss\n",
    "- Beta scheduling\n",
    "- Model checkpointing\n",
    "\n",
    "Excludes: classification and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcf0da78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspace\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "current = Path().resolve()\n",
    "while not (current / \"src\").exists():\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "print(\"Project root:\", current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8355cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.grayscale_datasets import get_grayscale_loader\n",
    "from src.models.vae import ConvVAE\n",
    "from src.training.losses import vae_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13893197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c5ca7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "batch_size = 32        # CPU-safe\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab548a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint dir: /workspace/checkpoints/grayscale\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = current / \"checkpoints\" / \"grayscale\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoint dir:\", ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "535e1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Training BASE VAE on MNIST\n",
      "==============================\n",
      "MNIST | Epoch [1/10] | Loss: 3046001\n",
      "MNIST | Epoch [2/10] | Loss: 3118639\n",
      "MNIST | Epoch [3/10] | Loss: 3823196\n",
      "MNIST | Epoch [4/10] | Loss: 3878317\n",
      "MNIST | Epoch [5/10] | Loss: 3790832\n",
      "MNIST | Epoch [6/10] | Loss: 3739327\n",
      "MNIST | Epoch [7/10] | Loss: 3693429\n",
      "MNIST | Epoch [8/10] | Loss: 3655617\n",
      "MNIST | Epoch [9/10] | Loss: 3625138\n",
      "MNIST | Epoch [10/10] | Loss: 3601726\n",
      "Saved → /workspace/checkpoints/grayscale/vae_mnist_64.pt\n",
      "\n",
      "==============================\n",
      "Training BASE VAE on FASHION\n",
      "==============================\n",
      "FASHION | Epoch [1/10] | Loss: 3513797\n",
      "FASHION | Epoch [2/10] | Loss: 3502546\n",
      "FASHION | Epoch [3/10] | Loss: 3915385\n",
      "FASHION | Epoch [4/10] | Loss: 3906579\n",
      "FASHION | Epoch [5/10] | Loss: 3823171\n",
      "FASHION | Epoch [6/10] | Loss: 3764531\n",
      "FASHION | Epoch [7/10] | Loss: 3723475\n",
      "FASHION | Epoch [8/10] | Loss: 3691465\n",
      "FASHION | Epoch [9/10] | Loss: 3664722\n",
      "FASHION | Epoch [10/10] | Loss: 3640680\n",
      "Saved → /workspace/checkpoints/grayscale/vae_fashion_64.pt\n",
      "\n",
      "==============================\n",
      "Training BASE VAE on EMNIST\n",
      "==============================\n",
      "EMNIST | Epoch [1/10] | Loss: 6816408\n",
      "EMNIST | Epoch [2/10] | Loss: 8227365\n",
      "EMNIST | Epoch [3/10] | Loss: 8224561\n",
      "EMNIST | Epoch [4/10] | Loss: 8063083\n",
      "EMNIST | Epoch [5/10] | Loss: 7965586\n",
      "EMNIST | Epoch [6/10] | Loss: 7887363\n",
      "EMNIST | Epoch [7/10] | Loss: 7833691\n",
      "EMNIST | Epoch [8/10] | Loss: 7791093\n",
      "EMNIST | Epoch [9/10] | Loss: 7762252\n",
      "EMNIST | Epoch [10/10] | Loss: 7725177\n",
      "Saved → /workspace/checkpoints/grayscale/vae_emnist_64.pt\n"
     ]
    }
   ],
   "source": [
    "datasets_to_train = [\"mnist\", \"fashion\", \"emnist\"]\n",
    "\n",
    "for ds in datasets_to_train:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Training BASE VAE on {ds.upper()}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    loader = get_grayscale_loader(\n",
    "    dataset_name=ds,\n",
    "    root=current / \"data\" / \"raw\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "    model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    beta_scheduler = BetaScheduler(\n",
    "        start=0.0,\n",
    "        end=1.0,\n",
    "        n_steps=5000\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x)\n",
    "\n",
    "            beta = beta_scheduler.step()\n",
    "            loss, _, _ = vae_loss(recon, x, mu, logvar, beta)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"{ds.upper()} | \"\n",
    "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "            f\"Loss: {total_loss:.0f}\"\n",
    "        )\n",
    "\n",
    "    ckpt_path = ckpt_dir / f\"vae_{ds}_64.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"Saved → {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "439c29b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Training SHARP VAE on ALL DATASETS\n",
      "==============================\n",
      "\n",
      "--- SHARP VAE on MNIST ---\n",
      "MNIST | Epoch [1/12] | Loss: 2759755 | Recon: 2496538 | KL: 5264341\n",
      "MNIST | Epoch [2/12] | Loss: 1215020 | Recon: 941084 | KL: 5478731\n",
      "MNIST | Epoch [3/12] | Loss: 1113698 | Recon: 840369 | KL: 5466592\n",
      "MNIST | Epoch [4/12] | Loss: 1059621 | Recon: 787309 | KL: 5446244\n",
      "MNIST | Epoch [5/12] | Loss: 1021342 | Recon: 750243 | KL: 5421996\n",
      "MNIST | Epoch [6/12] | Loss: 992118 | Recon: 722446 | KL: 5393444\n",
      "MNIST | Epoch [7/12] | Loss: 968967 | Recon: 700161 | KL: 5376114\n",
      "MNIST | Epoch [8/12] | Loss: 951817 | Recon: 683818 | KL: 5359982\n",
      "MNIST | Epoch [9/12] | Loss: 937565 | Recon: 670294 | KL: 5345433\n",
      "MNIST | Epoch [10/12] | Loss: 924583 | Recon: 657818 | KL: 5335300\n",
      "MNIST | Epoch [11/12] | Loss: 915095 | Recon: 648927 | KL: 5323354\n",
      "MNIST | Epoch [12/12] | Loss: 905336 | Recon: 639623 | KL: 5314251\n",
      "Saved SHARP → /workspace/checkpoints/grayscale/vae_mnist_sharp_64.pt\n",
      "\n",
      "--- SHARP VAE on FASHION ---\n",
      "FASHION | Epoch [1/12] | Loss: 3020905 | Recon: 2778740 | KL: 4843311\n",
      "FASHION | Epoch [2/12] | Loss: 1922835 | Recon: 1673836 | KL: 4979980\n",
      "FASHION | Epoch [3/12] | Loss: 1784209 | Recon: 1535459 | KL: 4975007\n",
      "FASHION | Epoch [4/12] | Loss: 1708311 | Recon: 1460079 | KL: 4964628\n",
      "FASHION | Epoch [5/12] | Loss: 1661062 | Recon: 1413760 | KL: 4946046\n",
      "FASHION | Epoch [6/12] | Loss: 1623802 | Recon: 1377022 | KL: 4935610\n",
      "FASHION | Epoch [7/12] | Loss: 1597358 | Recon: 1351030 | KL: 4926549\n",
      "FASHION | Epoch [8/12] | Loss: 1574646 | Recon: 1328881 | KL: 4915302\n",
      "FASHION | Epoch [9/12] | Loss: 1555683 | Recon: 1310511 | KL: 4903441\n",
      "FASHION | Epoch [10/12] | Loss: 1541268 | Recon: 1296527 | KL: 4894822\n",
      "FASHION | Epoch [11/12] | Loss: 1527936 | Recon: 1283353 | KL: 4891671\n",
      "FASHION | Epoch [12/12] | Loss: 1517140 | Recon: 1273032 | KL: 4882150\n",
      "Saved SHARP → /workspace/checkpoints/grayscale/vae_fashion_sharp_64.pt\n",
      "\n",
      "--- SHARP VAE on EMNIST ---\n",
      "EMNIST | Epoch [1/12] | Loss: 4056521 | Recon: 3527934 | KL: 10571746\n",
      "EMNIST | Epoch [2/12] | Loss: 2423827 | Recon: 1889456 | KL: 10687412\n",
      "EMNIST | Epoch [3/12] | Loss: 2269869 | Recon: 1738198 | KL: 10633413\n",
      "EMNIST | Epoch [4/12] | Loss: 2180963 | Recon: 1651322 | KL: 10592818\n",
      "EMNIST | Epoch [5/12] | Loss: 2127075 | Recon: 1599034 | KL: 10560821\n",
      "EMNIST | Epoch [6/12] | Loss: 2084525 | Recon: 1557167 | KL: 10547152\n",
      "EMNIST | Epoch [7/12] | Loss: 2056554 | Recon: 1529695 | KL: 10537185\n",
      "EMNIST | Epoch [8/12] | Loss: 2034774 | Recon: 1508090 | KL: 10533693\n",
      "EMNIST | Epoch [9/12] | Loss: 2015904 | Recon: 1489913 | KL: 10519818\n",
      "EMNIST | Epoch [10/12] | Loss: 2000813 | Recon: 1475249 | KL: 10511281\n",
      "EMNIST | Epoch [11/12] | Loss: 1989361 | Recon: 1464512 | KL: 10496987\n",
      "EMNIST | Epoch [12/12] | Loss: 1979183 | Recon: 1454659 | KL: 10490492\n",
      "Saved SHARP → /workspace/checkpoints/grayscale/vae_emnist_sharp_64.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==============================\")\n",
    "print(\"Training SHARP VAE on ALL DATASETS\")\n",
    "print(\"==============================\")\n",
    "\n",
    "datasets_to_train = [\"mnist\", \"fashion\", \"emnist\"]\n",
    "\n",
    "beta = 0.05          # LOW beta = sharper visuals\n",
    "epochs_sharp = 12    # enough for clarity, CPU-safe\n",
    "\n",
    "for ds in datasets_to_train:\n",
    "    print(f\"\\n--- SHARP VAE on {ds.upper()} ---\")\n",
    "\n",
    "    loader = get_grayscale_loader(\n",
    "        dataset_name=ds,\n",
    "        root=current / \"data\" / \"raw\",\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs_sharp):\n",
    "        total_loss = 0.0\n",
    "        total_recon = 0.0\n",
    "        total_kl = 0.0\n",
    "\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x)\n",
    "\n",
    "            loss, recon_loss, kl_loss = vae_loss(\n",
    "                recon, x, mu, logvar, beta\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_kl += kl_loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"{ds.upper()} | \"\n",
    "            f\"Epoch [{epoch+1}/{epochs_sharp}] | \"\n",
    "            f\"Loss: {total_loss:.0f} | \"\n",
    "            f\"Recon: {total_recon:.0f} | \"\n",
    "            f\"KL: {total_kl:.0f}\"\n",
    "        )\n",
    "\n",
    "    ckpt_path = ckpt_dir / f\"vae_{ds}_sharp_64.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"Saved SHARP → {ckpt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
