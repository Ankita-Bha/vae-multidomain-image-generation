# ============================================================
# Streamlit App: VAE Latent Space Exploration & Image Generator
#
# Purpose:
# 1. Show what the VAE learned (latent traversal)
# 2. Compare Vanilla VAE vs Sharp VAE vs VAE-GAN
# 3. Improve demo clarity via visualization-only upscaling
#
# NOTE:
# Images are generated at original resolution (28x28)
# and upscaled ONLY for display clarity.
#
# Run:
#   streamlit run app1.py
# ============================================================

import streamlit as st
import torch
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import torch.nn.functional as F

# ------------------------------------------------------------
# Project setup (make src/ importable)
# ------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.append(str(PROJECT_ROOT))

from src.models.vae import ConvVAE

# ------------------------------------------------------------
# Page config
# ------------------------------------------------------------
st.set_page_config(
    page_title="VAE Image Generator Demo",
    layout="wide"
)

st.title("ðŸ§  VAE Image Generator â€” Latent Space Exploration")
st.markdown(
    """
    This dashboard demonstrates **image generation using Variational Autoencoders**
    and shows how **visual quality improves** across model variants while keeping
    the same latent controls.

    **Important:**  
    Images are generated at **28Ã—28 resolution** (dataset-native) and **upscaled only
    for visualization clarity**.
    """
)

# ------------------------------------------------------------
# Sidebar controls
# ------------------------------------------------------------
st.sidebar.header("Controls")

dataset = st.sidebar.selectbox(
    "Select dataset",
    ["mnist", "fashion"]
)

model_type = st.sidebar.radio(
    "Select model type",
    [
        "Vanilla VAE (Structure)",
        "Sharp VAE (Better Visuals)",
        "VAE-GAN (Sharpest Generation)"
    ]
)

latent_dim = st.sidebar.slider(
    "Latent dimension to traverse",
    min_value=0,
    max_value=31,
    value=12
)

# ------------------------------------------------------------
# Model loader
# ------------------------------------------------------------
@st.cache_resource
def load_model(dataset_name, model_variant):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ConvVAE(latent_dim=32).to(device)

    if dataset_name == "mnist":
        ckpt_name = "vae_mnist.pt"
    else:
        if model_variant == "Vanilla VAE (Structure)":
            ckpt_name = "vae_fashion.pt"
        elif model_variant == "Sharp VAE (Better Visuals)":
            ckpt_name = "vae_fashion_sharp.pt"
        else:
            ckpt_name = "vae_fashion_gan.pt"

    ckpt_path = PROJECT_ROOT / "checkpoints" / "grayscale" / ckpt_name

    if not ckpt_path.exists():
        st.error(f"Checkpoint not found: {ckpt_path}")
        st.stop()

    model.load_state_dict(torch.load(ckpt_path, map_location=device))
    model.eval()
    return model, device, ckpt_name

model, device, ckpt_used = load_model(dataset, model_type)
st.caption(f"Using checkpoint: `{ckpt_used}`")

# ------------------------------------------------------------
# Latent traversal setup
# ------------------------------------------------------------
torch.manual_seed(42)  # fixed base latent (explainable demo)
z_base = torch.randn(1, 32).to(device)

traversal_values = [-3, -2, -1, 0, 1, 2, 3]
UPSCALE_FACTOR = 4  # 28x28 â†’ 112x112 (display only)

st.subheader(f"Latent Traversal â€” `{dataset.upper()}`")
st.markdown(
    f"""
    **Latent dimension:** `z[{latent_dim}]`

    Each image below is generated by modifying **only one latent variable**
    while keeping all others fixed.
    """
)

# ------------------------------------------------------------
# Generate traversal images (SAFE UPSCALING)
# ------------------------------------------------------------
images = []

with torch.no_grad():
    for val in traversal_values:
        z = z_base.clone()
        z[0, latent_dim] = val

        img = model.decoder(z)          # [1, 1, 28, 28]
        img = (img + 1) / 2             # normalize to [0, 1]

        # Upscale ONLY for visualization
        img_up = F.interpolate(
            img,
            scale_factor=UPSCALE_FACTOR,
            mode="bilinear",
            align_corners=False
        )

        images.append(img_up[0, 0].cpu().numpy())

# ------------------------------------------------------------
# Plot traversal grid (FIXED rendering)
# ------------------------------------------------------------
fig, axes = plt.subplots(1, len(traversal_values), figsize=(14, 3))

for i, val in enumerate(traversal_values):
    axes[i].imshow(images[i], cmap="gray", vmin=0, vmax=1)
    axes[i].set_title(str(val))
    axes[i].axis("off")

plt.suptitle(
    f"Effect of Traversing z[{latent_dim}] | {model_type}",
    fontsize=14
)

st.pyplot(fig, clear_figure=True)

# ------------------------------------------------------------
# Explanation block
# ------------------------------------------------------------
st.markdown(
    """
    ### ðŸ” How to interpret this demo

    - Images change **smoothly**, not randomly.
    - This shows the model learned a **continuous latent space**.
    - Each latent dimension captures a **visual factor** such as
      stroke thickness, curvature, or object silhouette.

    ### ðŸŽ¨ Why sharpness is limited

    - FashionMNIST and MNIST are **28Ã—28 grayscale datasets**
    - Fine texture does not exist in the data itself
    - VAE-GAN improves realism, but cannot exceed data resolution

    > Images are generated at **28Ã—28** and upscaled **only for display clarity**.

    ### ðŸ“Œ Key takeaway

    This system demonstrates:
    - True image generation
    - Interpretable latent controls
    - Progressive improvement from VAE â†’ VAE-GAN
    """
)

# ------------------------------------------------------------
# Future extension note
# ------------------------------------------------------------
with st.expander("ðŸ”® Future Extensions"):
    st.markdown(
        """
        Possible next steps:
        - Higher-resolution datasets (CelebA 64Ã—64)
        - Conditional VAEs
        - Diffusion models for realism comparison

        All extensions build on the same latent-space principles shown here.
        """
    )
