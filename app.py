# ============================================================
# Streamlit App: VAE Latent Space Exploration & Image Generator
#
# Purpose:
# 1. Show what the VAE learned (latent traversal)
# 2. Compare Vanilla VAE vs Sharp VAE vs VAE-GAN
# 3. Provide a strong, explainable image generation demo
#
# Run:
#   streamlit run app1.py
# ============================================================

import streamlit as st
import torch
import sys
from pathlib import Path
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Project setup (make src/ importable)
# ------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.append(str(PROJECT_ROOT))

from src.models.vae import ConvVAE

# ------------------------------------------------------------
# Page config
# ------------------------------------------------------------
st.set_page_config(
    page_title="VAE Image Generator Demo",
    layout="wide"
)

st.title("üß† VAE Image Generator ‚Äî Latent Space Exploration")
st.markdown(
    """
    This dashboard visualizes **what a Variational Autoencoder learns**
    and how **image quality improves** as we move from:
    
    - Vanilla VAE ‚Üí structure
    - Sharp VAE ‚Üí better reconstruction
    - VAE-GAN ‚Üí sharper generation

    The latent controls remain the same across all models.
    """
)

# ------------------------------------------------------------
# Sidebar controls
# ------------------------------------------------------------
st.sidebar.header("Controls")

dataset = st.sidebar.selectbox(
    "Select dataset",
    ["mnist", "fashion", "emnist"]
)

model_type = st.sidebar.radio(
    "Select model type",
    [
        "Vanilla VAE (Structure)",
        "Sharp VAE (Better Visuals)",
        "VAE-GAN (Sharpest Generation)"
    ]
)

latent_dim = st.sidebar.slider(
    "Latent dimension to traverse",
    min_value=0,
    max_value=31,
    value=12
)

# ------------------------------------------------------------
# Model loader
# ------------------------------------------------------------
@st.cache_resource
def load_model(dataset_name, model_variant):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ConvVAE(latent_dim=32).to(device)

    # Decide checkpoint
    if dataset_name != "fashion":
        ckpt_name = f"vae_{dataset_name}.pt"
    else:
        if model_variant == "Vanilla VAE (Structure)":
            ckpt_name = "vae_fashion.pt"
        elif model_variant == "Sharp VAE (Better Visuals)":
            ckpt_name = "vae_fashion_sharp.pt"
        else:
            ckpt_name = "vae_fashion_gan.pt"

    ckpt_path = PROJECT_ROOT / "checkpoints" / "grayscale" / ckpt_name

    if not ckpt_path.exists():
        st.error(f"Checkpoint not found: {ckpt_path}")
        st.stop()

    model.load_state_dict(torch.load(ckpt_path, map_location=device))
    model.eval()
    return model, device, ckpt_name

model, device, ckpt_used = load_model(dataset, model_type)
st.caption(f"Using checkpoint: `{ckpt_used}`")

# ------------------------------------------------------------
# Latent traversal setup
# ------------------------------------------------------------
torch.manual_seed(42)  # fixed base latent for explainability
z_base = torch.randn(1, 32).to(device)

traversal_values = [-3, -2, -1, 0, 1, 2, 3]

st.subheader(f"Latent Traversal ‚Äî `{dataset.upper()}`")
st.markdown(
    f"""
    **Latent dimension:** `z[{latent_dim}]`

    Each image below is generated by modifying **only this one latent variable**
    while keeping all others fixed.
    """
)

# ------------------------------------------------------------
# Generate traversal images (UPSCALED FOR DISPLAY)
# ------------------------------------------------------------
import torch.nn.functional as F

images = []
UPSCALE_FACTOR = 4  # 28x28 ‚Üí 112x112 (visual only)

with torch.no_grad():
    for val in traversal_values:
        z = z_base.clone()
        z[0, latent_dim] = val

        img = model.decoder(z)          # [1, 1, 28, 28]
        img = (img + 1) / 2             # de-normalize to [0, 1]

        # Upscale ONLY for visualization
        img_up = F.interpolate(
            img,
            scale_factor=UPSCALE_FACTOR,
            mode="bilinear",
            align_corners=False
        )

        images.append(img_up[0, 0].cpu())

# ------------------------------------------------------------
# Explanation block (demo-critical)
# ------------------------------------------------------------
st.markdown(
    """
    ### üîç How to interpret this demo

    - Images change **smoothly**, indicating a learned continuous latent space.
    - Each latent dimension captures a **visual factor** (shape, thickness, silhouette).

    ### üé® Model differences

    **Vanilla VAE**  
    - Strong latent structure  
    - Blurry generation due to pixel-wise loss  

    **Sharp VAE (low Œ≤)**  
    - Better reconstruction  
    - Same latent control  

    **VAE-GAN**  
    - Adversarial training improves realism  
    - Sharpest generated images  
    - Latent space behavior preserved  

    ### üìå Key takeaway

    This system is a **true image generator** that demonstrates
    how generative models trade off **interpretability vs realism**.
    """
)

# ------------------------------------------------------------
# Future extension note
# ------------------------------------------------------------
with st.expander("üîÆ Future Extensions (CelebA / Higher Resolution)"):
    st.markdown(
        """
        Possible next steps:
        - CelebA face generation
        - Higher resolution VAEs
        - Conditional VAEs
        - Diffusion model comparison

        All would reuse the same latent analysis concepts shown here.
        """
    )
